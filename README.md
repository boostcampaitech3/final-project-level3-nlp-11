# ğŸ• ë¦¬íŠ¸ë¦¬ë²„
<div align="center">
  <img src="https://shields.io/badge/python-v3.8.5-blue?logo=python" />
  <img src="https://shields.io/badge/pytorch-v1.10.2-red?logo=pytorch" />
  <img src="https://img.shields.io/badge/transformers-4.10.0-yellow" />
  <img src="https://img.shields.io/badge/Streamlit-red" />
  <img src="https://img.shields.io/badge/FastAPI-blue" />
</div>

<br />

<br />

# 1. í”„ë¡œì íŠ¸ ê°œìš”

ğŸ• ë¦¬íŠ¸ë¦¬ë²„ëŠ” **ì‹œê°ì¥ì• ì¸ì„ ìœ„í•˜ì—¬ ì‹œì•¼ë¥¼ ìŒì„±ìœ¼ë¡œ ì•ˆë‚´í•˜ëŠ” ì„œë¹„ìŠ¤**ì…ë‹ˆë‹¤.
ì‹œê°ì¥ì• ì¸ì´ ê¸°ì¡´ì˜ ë³´ì¡°ê¸°êµ¬ë¡œ ì‹œì•¼ë¥¼ ì¸ì§€í•  ìˆ˜ ì—†ì„ ë•Œ, ì‚¬ì§„ì„ ì°ì–´ ìƒí™©ì— ëŒ€í•œ ì„¤ëª…ì´ ìŒì„±ìœ¼ë¡œ ì œê³µ(Image Captioning)ë˜ëŠ” ê¸°ëŠ¥ê³¼ ì´ë¯¸ì§€ì— ëŒ€í•œ ë¶€ê°€ ì„¤ëª…ì´ í•„ìš”í•˜ë‹¤ë©´ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µ(Visual Question Answering)ì´ ì œê³µë©ë‹ˆë‹¤.

<div align="center">
  <img src="https://github.com/boostcampaitech3/final-project-level3-nlp-11/blob/main/assets/img2.png?raw=true">
</div>

ìŒí–¥ì‹ í˜¸ê¸°ê°€ ì—†ëŠ” ì‹ í˜¸ë“±ê³¼ ë¬´ì‹ í˜¸ íš¡ë‹¨ë³´ë„ëŠ” ì‹œê°ì¥ì• ì¸ì˜ ë³´í–‰ì„ ì–´ë µê²Œ í•˜ëŠ” ë§ì€ ìš”ì†Œ ì¤‘ í•˜ë‚˜ ì…ë‹ˆë‹¤. ë¬´ì‹ í˜¸ íš¡ë‹¨ë³´ë„ ë¿ë§Œ ì•„ë‹ˆë¼ ë„ë¡œ ìœ„ì˜ ì¥ì• ë¬¼, ê¹¨ì ¸ ìˆëŠ” ì—°ì„ ë“± ì‹œê°ì¥ì• ì¸ì´ ë³´í–‰ì„ í•˜ëŠ”ë° ìœ„í˜‘ì´ ë˜ëŠ” ìš”ì†Œê°€ ê³³ê³³ì— ì¡´ì¬í•©ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê³ ì í•´ë‹¹ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.

**AIë¥¼ ì´ìš©í•˜ì—¬ ì‚¬íšŒì— ê¸ì •ì ì¸ ë°©í–¥ìœ¼ë¡œ ê¸°ì—¬**í•˜ê³ , **ì‹œê°ì¥ì• ì¸ë“¤ì˜ ì™¸ë¶€ í™œë™ì„ ë³´ì¡°í•˜ê¸° ìœ„í•œ ë„êµ¬ë¡œì„œ ê¸°ëŠ¥**ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br />

<br />

# 2. ì‚¬ìš© ë°©ë²•

## **Image Captioning**

**Image Captioning model**

Windows :Â [download](https://drive.google.com/file/d/1_sQt__98URycUtygbjHZ3Th8hpGTx806/view?usp=sharing)

```bash
# linux
$ wget --no-check-certificate --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1_sQt__98URycUtygbjHZ3Th8hpGTx806' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1_sQt__98URycUtygbjHZ3Th8hpGTx806" -O quantized_model.pth.tar && rm -rf /tmp/cookies.txt
```

<br />

<br />

## VQA(**Visual Question Answering)**

### **VQA model**

Windows :Â [download](https://drive.google.com/file/d/1Ttz8DLQl9ZxrwLznJwNLcutr2hhZxZOk/view?usp=sharing)

```bash
# linux
$ wget --no-check-certificate --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Ttz8DLQl9ZxrwLznJwNLcutr2hhZxZOk' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1Ttz8DLQl9ZxrwLznJwNLcutr2hhZxZOk" -O vqa_demo_epoch1.pth && rm -rf /tmp/cookies.txt
```

<br />

<br />

### **Label to Answer**

Windows :Â [download](https://drive.google.com/file/d/1truanDe3btDUFKBtoxKpVux7jx3cyTtI/view?usp=sharing)

```bash
# linux
$ wget --no-check-certificate --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1truanDe3btDUFKBtoxKpVux7jx3cyTtI' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1truanDe3btDUFKBtoxKpVux7jx3cyTtI" -O trainval_label2ans.kvqa.pkl && rm -rf /tmp/cookies.txt
```

<br />

<br />

## **Deploy**

### **FastAPI**

```bash
$ uvicorn main:app --host 0.0.0.0 --port PORT
```

### **Streamlit**

```bash
$ streamlit run streamlit_deploy.py --server.port PORT --browser.serverAddress 0.0.0.0
```

<br />

<br />

# 3. í”„ë¡œì íŠ¸ ëª¨ë¸

## Image Captioning

<p align="center">
  <img src="https://github.com/boostcampaitech3/final-project-level3-nlp-11/blob/main/assets/Untitled%201.png?raw=true">
</p>

- *Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering*ì—ì„œ ì œì•ˆí•œ Modelì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
- Bottom-Up and Top-Down Attention Modelì€ ì´ë¯¸ì§€ ë‚´ ê°ì²´ë¥¼ íƒì§€í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì¸ Faster-RCNNì„ ì‚¬ìš©í•˜ì—¬ í›„ë³´ì˜ì—­ ì¸ ROIë¥¼ ì¶”ì¶œí•˜ê³ , ì´ ROIê°€ ê°ì²´ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ë°˜ì˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
- Decoderì˜ ì²«ë²ˆì§¸ Layerì¸ Top-Down Attention LSTMì—ì„œëŠ” Hidden stateì™€ Visual eatureì˜ í‰ê·  ê·¸ë¦¬ê³  Word embedding ê°’ì„ Concatí•œ Vectorë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, ê¸°ì¡´ Baseline(Show, Attend and Tell)ì´ ìˆ˜í–‰í•˜ë˜ Top-down Attention ì—­í• ì„ í•©ë‹ˆë‹¤.
- Top-Down Attention LSTMì˜ Output hidden stateëŠ” ROI Featureì™€ Alignì„ ìˆ˜í–‰ í›„ Attention vectorì™€ Concat í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ Language LSTM Layerì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë˜ê³ , Language LSTMì—ì„œëŠ” ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ Captionì„ ìƒì„±í•©ë‹ˆë‹¤.

## VQA

<p align="center">
  <img src="https://github.com/boostcampaitech3/final-project-level3-nlp-11/raw/main/assets/Untitled%202.png?raw=true">
</p>

- *VisualBERT: A Simple and Performant Baseline for Vision and Language*ì—ì„œ ì œì•ˆí•œ Modelì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
- VisualBERTëŠ” Faster RCNN ì„ ì´ìš©í•œ Bottom-upêµ¬ì¡°ì™€, VilBERTë¥¼ ì´ìš©í•œ Top-down êµ¬ì¡°ë¡œ
ì´ë¯¸ì§€ë¥¼ ì§ˆë¬¸ì— ëŒ€í•˜ì—¬ ì„¸ì„¸í•˜ê²Œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- Faster RCNNìœ¼ë¡œ ì¶”ì¶œëœ ROI featureì„ Question embeddingê³¼ concatenate í›„ ëª¨ë¸ì— ì…ë ¥ë˜ì–´
ì´ë¯¸ì§€ì™€ í•¨ê»˜ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.
- ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” VQAë¥¼ Multiple-Choice Taskë¡œ ì ‘ê·¼í•˜ì˜€ê¸° ë•Œë¬¸ì— outputì„ Classification layerë¥¼ í†µí•˜ì—¬ answerë¥¼ ì¶”ì¶œí•˜ì˜€ìŠµë‹ˆë‹¤.

<br />

<br />

# 4. í”„ë¡œì íŠ¸ ê²°ê³¼

### Image Caption

|                   Model Name |                     Tokenizer |       BLEU-4 |
| --- | --- | --- |
| Baseline(ResNet101 + LSTM with Attention) | ì–´ì ˆ ë‹¨ìœ„ í† í¬ë‚˜ì´ì§• |         30.27 |
| Baseline(ResNet101 + LSTM with Attention) | Tokenizer(â€monologg/kobigbird-bert-baseâ€) |         33.49 |
| Baseline(ResNet101 + LSTM with Attention) | Tokenizer(â€monologg/kobigbird-bert-baseâ€) |         34.01 |
| Baseline(ResNet101 + LSTM with Attention) | Tokenizer(â€monologg/kobigbird-bert-baseâ€) |         34.17 |
| BUTD(Bottom-Up and Top-Down Attention) | Tokenizer(â€monologg/kobigbird-bert-baseâ€) |         38.73 |

  
### VQA

| Model Name | Tokenizer | ACC |
| --- | --- | --- |
| ConvNext_Large+LSTM(over 6 same responses) | Tokenizer(â€klue/bert-baseâ€) | 0.53 |
| VisualBert(with Faster RCNN)(over 6 same responses) | Tokenizer(â€klue/bert-baseâ€) | 0.58 |
| VisualBert(with Faster RCNN, additional dataset)(over 6 same responses) | Tokenizer(â€klue/bert-baseâ€) | 0.64 |

<br />

<br />

# 5. ì‹œì—° ì˜ˆì‹œ

ì‹¤ì œ ì‹œì—° ì˜ìƒì€ [ì´ê³³](https://youtube.com/shorts/AVXBsLZOkvE?feature=share)ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<div align="center">
  <img src="https://github.com/boostcampaitech3/final-project-level3-nlp-11/blob/main/assets/img4.png?raw=true">
</div>

<br />

<br />

# 6. í”„ë¡œì íŠ¸ íŒ€ êµ¬ì„± ë° ì—­í• 

| ì´ë¦„ | ì—­í•  |
| --- | --- |
| ê¹€ë•ë˜ | Image Captioning |
| ê¹€ìš©í¬ | VQA |
| ì´ë‘í˜¸ | Image Captioning(ClipCap) / Streamlit / FastAPI |
| ì´ìŠ¹í™˜ | VQA |
| ì¡°í˜ì¤€ | Image Captioning |
| ìµœì§„í˜ | VQA |

<br />

<br />

# Reference

### Datasets

**Image Captioning**

- **[MSCOCO ì‚¬ì§„](https://cocodataset.org/)**
- **[MSCOCO í•œê¸€ ì£¼ì„](https://aihub.or.kr/opendata/keti-data/recognition-visual/KETI-01-003)**

**VQA**

- **[SKT BRAIN - KVQA](https://sktbrain.github.io/KVQA/)**
- **[ì…€ë ‰íŠ¸ìŠ¤íƒ€ - êµì°¨ë¡œ ì •ë³´ ë°ì´í„°ì…‹](https://open.selectstar.ai/data-set/wesee)**
- **[AI HUB - ìƒí™œ ë° ê±°ì£¼í™˜ê²½ ê¸°ë°˜ VQA](https://aihub.or.kr/aidata/34147)**

### Papers

**Image Captioning**

- ****[Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044)****
- ****[Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning](https://arxiv.org/abs/1612.01887)****
- ****[Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](https://arxiv.org/abs/1707.07998)****

**VQA**

- ****[VQA: Visual Question Answering(2016)](https://arxiv.org/abs/1505.00468)****
- ****[VisualBERT: A Simple and Performant Baseline for Vision and Language(2019)](https://arxiv.org/abs/1908.03557)****
- **[SKTBrain / BAN-KVQA](https://github.com/SKTBrain/BAN-KVQA)**
